Time elapsed on feature extraction: 0.0
Time elapsed on reading serialized features: 5.75588059425354
MLPClassifier(activation='tanh', alpha=0.0001, batch_size='auto', beta_1=0.9,
       beta_2=0.999, early_stopping=False, epsilon=1e-08,
       hidden_layer_sizes=(100, 100), learning_rate='constant',
       learning_rate_init=0.001, max_iter=200, momentum=0.9,
       nesterovs_momentum=True, power_t=0.5, random_state=None,
       shuffle=True, solver='sgd', tol=0.0, validation_fraction=0.1,
       verbose=True, warm_start=False)
Iteration 1, loss = 0.66924545
Iteration 2, loss = 0.65125590
Iteration 3, loss = 0.64839834
Iteration 4, loss = 0.64541814
Iteration 5, loss = 0.64318514
Iteration 6, loss = 0.64087250
Iteration 7, loss = 0.63754467
Iteration 8, loss = 0.63644990
Iteration 9, loss = 0.63531458
Iteration 10, loss = 0.63464671
Iteration 11, loss = 0.63391835
Iteration 12, loss = 0.63366568
Iteration 13, loss = 0.63219433
Iteration 14, loss = 0.63170326
Iteration 15, loss = 0.63168399
Iteration 16, loss = 0.63180842
Iteration 17, loss = 0.63068956
Iteration 18, loss = 0.63004974
Iteration 19, loss = 0.62960486
Iteration 20, loss = 0.62967761
Iteration 21, loss = 0.62883078
Iteration 22, loss = 0.62903198
Iteration 23, loss = 0.62830636
Iteration 24, loss = 0.62782898
Iteration 25, loss = 0.62757565
Iteration 26, loss = 0.62695161
Iteration 27, loss = 0.62698086
Iteration 28, loss = 0.62674475
Iteration 29, loss = 0.62616918
Iteration 30, loss = 0.62567654
Iteration 31, loss = 0.62475605
Iteration 32, loss = 0.62495346
Iteration 33, loss = 0.62438067
Iteration 34, loss = 0.62398412
Iteration 35, loss = 0.62332401
Iteration 36, loss = 0.62359750
Iteration 37, loss = 0.62309807
Iteration 38, loss = 0.62311075
Iteration 39, loss = 0.62277775
Iteration 40, loss = 0.62251981
Iteration 41, loss = 0.62138667
Iteration 42, loss = 0.62130583
Iteration 43, loss = 0.62195804
Iteration 44, loss = 0.62123260
Iteration 45, loss = 0.62066275
Iteration 46, loss = 0.62058535
Iteration 47, loss = 0.62081433
Iteration 48, loss = 0.61995703
Iteration 49, loss = 0.62016472
Iteration 50, loss = 0.61902098
Iteration 51, loss = 0.61840143
Iteration 52, loss = 0.61808296
Iteration 53, loss = 0.61829634
Iteration 54, loss = 0.61782081
Iteration 55, loss = 0.61790268
Iteration 56, loss = 0.61721045
Iteration 57, loss = 0.61720326
Iteration 58, loss = 0.61657707
Iteration 59, loss = 0.61630067
Iteration 60, loss = 0.61651043
Iteration 61, loss = 0.61581381
Iteration 62, loss = 0.61565186
Iteration 63, loss = 0.61513206
Iteration 64, loss = 0.61597526
Iteration 65, loss = 0.61514682
Iteration 66, loss = 0.61505759
Iteration 67, loss = 0.61450572
Iteration 68, loss = 0.61425205
Iteration 69, loss = 0.61453278
Iteration 70, loss = 0.61429246
Iteration 71, loss = 0.61348855
Iteration 72, loss = 0.61440118
Iteration 73, loss = 0.61322794
Iteration 74, loss = 0.61275647
Iteration 75, loss = 0.61280713
Iteration 76, loss = 0.61263851
Iteration 77, loss = 0.61271653
Iteration 78, loss = 0.61206139
Iteration 79, loss = 0.61225534
Iteration 80, loss = 0.61161013
Iteration 81, loss = 0.61251976
Iteration 82, loss = 0.61185299
Iteration 83, loss = 0.61147338
Iteration 84, loss = 0.61098446
Iteration 85, loss = 0.61116801
Iteration 86, loss = 0.61088602
Iteration 87, loss = 0.60986500
Iteration 88, loss = 0.60963725
Iteration 89, loss = 0.60914880
Iteration 90, loss = 0.61015627
Iteration 91, loss = 0.60961930
Iteration 92, loss = 0.60920464
Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.
Time elapsed on network learning: 24.656136512756348
