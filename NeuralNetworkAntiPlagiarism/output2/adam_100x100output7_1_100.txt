Time elapsed on feature extraction: 0.0
Time elapsed on reading serialized features: 8.433975219726562
MLPClassifier(activation='tanh', alpha=0.0001, batch_size='auto', beta_1=0.9,
       beta_2=0.999, early_stopping=False, epsilon=1e-08,
       hidden_layer_sizes=(100, 100), learning_rate='constant',
       learning_rate_init=0.001, max_iter=200, momentum=0.9,
       nesterovs_momentum=True, power_t=0.5, random_state=None,
       shuffle=True, solver='adam', tol=0.0, validation_fraction=0.1,
       verbose=True, warm_start=False)
Iteration 1, loss = 0.30527948
Iteration 2, loss = 0.26668716
Iteration 3, loss = 0.25137959
Iteration 4, loss = 0.23635387
Iteration 5, loss = 0.23055437
Iteration 6, loss = 0.22742533
Iteration 7, loss = 0.22524565
Iteration 8, loss = 0.22406422
Iteration 9, loss = 0.22285285
Iteration 10, loss = 0.22164855
Iteration 11, loss = 0.22081115
Iteration 12, loss = 0.21996790
Iteration 13, loss = 0.21876510
Iteration 14, loss = 0.21853912
Iteration 15, loss = 0.21743173
Iteration 16, loss = 0.21749421
Iteration 17, loss = 0.21673270
Iteration 18, loss = 0.21585490
Iteration 19, loss = 0.21537352
Iteration 20, loss = 0.21610912
Iteration 21, loss = 0.21497973
Iteration 22, loss = 0.21433722
Iteration 23, loss = 0.21390445
Iteration 24, loss = 0.21451924
Iteration 25, loss = 0.21365124
Iteration 26, loss = 0.21302359
Iteration 27, loss = 0.21342824
Iteration 28, loss = 0.21225334
Iteration 29, loss = 0.21336485
Iteration 30, loss = 0.21165138
Iteration 31, loss = 0.21082631
Iteration 32, loss = 0.21126541
Iteration 33, loss = 0.21150757
Iteration 34, loss = 0.20995958
Iteration 35, loss = 0.20963220
Iteration 36, loss = 0.20884113
Iteration 37, loss = 0.20865047
Iteration 38, loss = 0.20831963
Iteration 39, loss = 0.20831638
Iteration 40, loss = 0.20734686
Iteration 41, loss = 0.20751380
Iteration 42, loss = 0.20745540
Iteration 43, loss = 0.20589450
Iteration 44, loss = 0.20554726
Iteration 45, loss = 0.20528107
Iteration 46, loss = 0.20523194
Iteration 47, loss = 0.20435672
Iteration 48, loss = 0.20400340
Iteration 49, loss = 0.20415771
Iteration 50, loss = 0.20319443
Iteration 51, loss = 0.20292730
Iteration 52, loss = 0.20295228
Iteration 53, loss = 0.20181544
Iteration 54, loss = 0.20180965
Iteration 55, loss = 0.20067197
Iteration 56, loss = 0.20099842
Iteration 57, loss = 0.20054696
Iteration 58, loss = 0.20001792
Iteration 59, loss = 0.20022280
Iteration 60, loss = 0.19920603
Iteration 61, loss = 0.19881416
Iteration 62, loss = 0.19860111
Iteration 63, loss = 0.19718539
Iteration 64, loss = 0.19778749
Iteration 65, loss = 0.19702439
Iteration 66, loss = 0.19700442
Iteration 67, loss = 0.19705955
Iteration 68, loss = 0.19602354
Iteration 69, loss = 0.19597898
Iteration 70, loss = 0.19557032
Iteration 71, loss = 0.19555842
Iteration 72, loss = 0.19553925
Iteration 73, loss = 0.19401513
Iteration 74, loss = 0.19392009
Iteration 75, loss = 0.19441109
Iteration 76, loss = 0.19318267
Iteration 77, loss = 0.19256016
Iteration 78, loss = 0.19221890
Iteration 79, loss = 0.19196239
Iteration 80, loss = 0.19041247
Iteration 81, loss = 0.19073874
Iteration 82, loss = 0.18994014
Iteration 83, loss = 0.18967513
Iteration 84, loss = 0.18861112
Iteration 85, loss = 0.18892090
Iteration 86, loss = 0.18786605
Iteration 87, loss = 0.18747171
Iteration 88, loss = 0.18767716
Iteration 89, loss = 0.18683986
Iteration 90, loss = 0.18589186
Iteration 91, loss = 0.18510208
Iteration 92, loss = 0.18485633
Iteration 93, loss = 0.18426454
Iteration 94, loss = 0.18366663
Iteration 95, loss = 0.18379104
Iteration 96, loss = 0.18356223
Iteration 97, loss = 0.18255562
Iteration 98, loss = 0.18258271
Iteration 99, loss = 0.18254125
Iteration 100, loss = 0.18090161
Iteration 101, loss = 0.18191688
Iteration 102, loss = 0.18105277
Iteration 103, loss = 0.18025037
Iteration 104, loss = 0.18005572
Iteration 105, loss = 0.17922958
Iteration 106, loss = 0.17904135
Iteration 107, loss = 0.17911807
Iteration 108, loss = 0.17790550
Iteration 109, loss = 0.17879536
Iteration 110, loss = 0.17738704
Iteration 111, loss = 0.17763417
Iteration 112, loss = 0.17680165
Iteration 113, loss = 0.17624516
Iteration 114, loss = 0.17628349
Iteration 115, loss = 0.17519059
Iteration 116, loss = 0.17648745
Iteration 117, loss = 0.17614894
Iteration 118, loss = 0.17414712
Iteration 119, loss = 0.17400573
Iteration 120, loss = 0.17382364
Iteration 121, loss = 0.17397849
Iteration 122, loss = 0.17315994
Iteration 123, loss = 0.17244788
Iteration 124, loss = 0.17277926
Iteration 125, loss = 0.17226147
Iteration 126, loss = 0.17232834
Iteration 127, loss = 0.17108855
Iteration 128, loss = 0.17129941
Iteration 129, loss = 0.17058736
Iteration 130, loss = 0.17120049
Iteration 131, loss = 0.17053140
Iteration 132, loss = 0.16980871
Iteration 133, loss = 0.17014239
Iteration 134, loss = 0.16915083
Iteration 135, loss = 0.16979142
Iteration 136, loss = 0.16884940
Iteration 137, loss = 0.16856313
Iteration 138, loss = 0.16799453
Iteration 139, loss = 0.16908715
Iteration 140, loss = 0.16799489
Iteration 141, loss = 0.16730499
Iteration 142, loss = 0.16723539
Iteration 143, loss = 0.16709039
Iteration 144, loss = 0.16688821
Iteration 145, loss = 0.16651731
Iteration 146, loss = 0.16692234
Iteration 147, loss = 0.16618435
Iteration 148, loss = 0.16592201
Iteration 149, loss = 0.16547896
Iteration 150, loss = 0.16567797
Iteration 151, loss = 0.16536199
Iteration 152, loss = 0.16513094
Iteration 153, loss = 0.16592612
Iteration 154, loss = 0.16568797
Iteration 155, loss = 0.16408238
Iteration 156, loss = 0.16402259
Iteration 157, loss = 0.16347983
Iteration 158, loss = 0.16311425
Iteration 159, loss = 0.16349876
Iteration 160, loss = 0.16439347
Iteration 161, loss = 0.16290128
Iteration 162, loss = 0.16298003
Iteration 163, loss = 0.16290964
Iteration 164, loss = 0.16260184
Iteration 165, loss = 0.16196632
Iteration 166, loss = 0.16163173
Iteration 167, loss = 0.16061234
Iteration 168, loss = 0.16088549
Iteration 169, loss = 0.16043215
Iteration 170, loss = 0.16064889
Iteration 171, loss = 0.16114660
Iteration 172, loss = 0.16113246
Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.
Time elapsed on network learning: 72.57263445854187
